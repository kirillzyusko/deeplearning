{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "lab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "WZfImSn8R0d0",
        "colab_type": "text"
      },
      "source": [
        "<b>Google Colab</b> <a href=\"https://colab.research.google.com/github/kirillzyusko/deeplearning/blob/master/7/lab.ipynb\">link</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml1ygy5_uBJS",
        "colab_type": "text"
      },
      "source": [
        "Authorize google + kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKErrc71uBQb",
        "colab_type": "code",
        "outputId": "6ed23f91-27bc-44be-b03e-8de81f66e09c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "190Qk0KuuUSa",
        "colab_type": "text"
      },
      "source": [
        "Be sure, that we authorized and have an access to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYY0Nxi_uUZO",
        "colab_type": "code",
        "outputId": "e9766c63-aea2-4045-b474-0a9bf6c7b138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls /content/.kaggle/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;32mkaggle.json\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g0sOvfQO1nc",
        "colab_type": "text"
      },
      "source": [
        "# **Part 1: Download dataset, extract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Xdl2_LPDxg",
        "colab_type": "text"
      },
      "source": [
        "Download dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cca54DmnPD5S",
        "colab_type": "code",
        "outputId": "800f879b-60b6-427b-cc83-6313b0846b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews -p /content/kaggle/imdb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content/kaggle/imdb\n",
            " 35% 9.00M/25.7M [00:00<00:01, 11.7MB/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 35.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2hNyXbPvSQa",
        "colab_type": "text"
      },
      "source": [
        "Extract .zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99z2-nHFvT5B",
        "colab_type": "code",
        "outputId": "c3077dcc-1773-47bf-9c3e-1ce42f60095e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip kaggle/imdb/imdb-dataset-of-50k-movie-reviews.zip -d data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  kaggle/imdb/imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: data/IMDB Dataset.csv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3LwHVDEwcLD",
        "colab_type": "text"
      },
      "source": [
        "Files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeETnYp5wby8",
        "colab_type": "code",
        "outputId": "7b13adc3-56e4-48f3-9a99-4d053ac4c45f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'IMDB Dataset.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8UdGt-Pw8Mf",
        "colab_type": "text"
      },
      "source": [
        "Read data using pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "489LXKpvw8S0",
        "colab_type": "code",
        "outputId": "87eef6f9-463c-4a82-d82c-9e98cfa5409e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data/IMDB Dataset.csv')\n",
        "print(df.shape)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBnNRwRS1dRs",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caAMPNsZv575",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mgqRl7JwOY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary_length = 10000\n",
        "input_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=dictionary_length)\n",
        "tokenizer.fit_on_texts(df.review.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmPueIwywc3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1624cdef-20e4-48ac-9241-bbb946d899d4"
      },
      "source": [
        "post_seq = tokenizer.texts_to_sequences(df.review.values)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 124252 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLRPD1rdwgE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "post_seq_padded = pad_sequences(post_seq, maxlen=input_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1iluafsxFXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_original = post_seq_padded\n",
        "x_original = np.array(x_original)\n",
        "\n",
        "y_original = df['sentiment'].replace({ 'positive': 1, 'negative': 0 }).values\n",
        "y_original = np.array(y_original)\n",
        "\n",
        "x, y = shuffle(x_original, y_original, random_state=23)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beAbDT-vxM6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6etXKKLfO2h5",
        "colab_type": "text"
      },
      "source": [
        "# **Part 2: RNN with LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtE3byXjz7lR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmeTQ6tcxVpo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8368e298-f5cb-4d5e-e0fc-7a788cae2d33"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(dictionary_length, 8, input_length=input_length))\n",
        "model.add(Bidirectional(LSTM(16, return_sequences=False))) # dropout=0.2, recurrent_dropout=0.2\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 100, 8)            80000     \n",
            "_________________________________________________________________\n",
            "bidirectional_18 (Bidirectio (None, 32)                3200      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 83,233\n",
            "Trainable params: 83,233\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBPVAf9VxoBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sznaUdIBxtwI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "94f72c01-7fd9-4489-e1f7-4396015cca07"
      },
      "source": [
        "model.fit(x=x_train, y=y_train, batch_size=256, verbose=1, epochs=10, validation_data=(x_val, y_val))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "79/79 [==============================] - 2s 22ms/step - loss: 0.6516 - accuracy: 0.6165 - val_loss: 0.4897 - val_accuracy: 0.7408\n",
            "Epoch 2/10\n",
            "79/79 [==============================] - 1s 14ms/step - loss: 0.3705 - accuracy: 0.8431 - val_loss: 0.4038 - val_accuracy: 0.8268\n",
            "Epoch 3/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.2742 - accuracy: 0.8924 - val_loss: 0.3578 - val_accuracy: 0.8528\n",
            "Epoch 4/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.2252 - accuracy: 0.9172 - val_loss: 0.4646 - val_accuracy: 0.8116\n",
            "Epoch 5/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.1945 - accuracy: 0.9288 - val_loss: 0.4152 - val_accuracy: 0.8450\n",
            "Epoch 6/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.1591 - accuracy: 0.9445 - val_loss: 0.4120 - val_accuracy: 0.8424\n",
            "Epoch 7/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.1351 - accuracy: 0.9560 - val_loss: 0.4886 - val_accuracy: 0.8356\n",
            "Epoch 8/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.1186 - accuracy: 0.9608 - val_loss: 0.5074 - val_accuracy: 0.8394\n",
            "Epoch 9/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.1015 - accuracy: 0.9682 - val_loss: 0.5249 - val_accuracy: 0.8326\n",
            "Epoch 10/10\n",
            "79/79 [==============================] - 1s 15ms/step - loss: 0.0912 - accuracy: 0.9732 - val_loss: 0.5607 - val_accuracy: 0.8320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f47d0ec9c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVKqkwetxxHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "78fe3416-72f5-425c-e1b5-5f8786719baf"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 4s 6ms/step - loss: 0.5428 - accuracy: 0.8340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.542818009853363, 0.8339599967002869]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIrvEkMaO2uP",
        "colab_type": "text"
      },
      "source": [
        "# **Part 3: Using GloVe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlaKV6VOPFCI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MpP3hLeAiT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "cc12e055-3447-450b-bf3a-e0c666289349"
      },
      "source": [
        "!wget \"http://nlp.stanford.edu/data/glove.6B.zip\""
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-03 21:06:25--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-03 21:06:26--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-03 21:06:26--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.02MB/s    in 6m 39s  \n",
            "\n",
            "2020-04-03 21:13:06 (2.06 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gADgatQ0A0sv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "945fb51d-453b-4a42-8fea-f4e3fa0609f1"
      },
      "source": [
        "!mkdir glove\n",
        "!unzip glove.6B.zip -d glove"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove/glove.6B.50d.txt  \n",
            "  inflating: glove/glove.6B.100d.txt  \n",
            "  inflating: glove/glove.6B.200d.txt  \n",
            "  inflating: glove/glove.6B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtWg-lOaCHPH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66e6ab67-c8c2-43e9-95a4-28ecf9f1c520"
      },
      "source": [
        "glove_dir = 'glove'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YheEVG1CCPIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((dictionary_length, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < dictionary_length:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXW2oZM9CjIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "6220dca9-a488-4876-d53a-7a509b7b8998"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(dictionary_length, embedding_dim, input_length=input_length))\n",
        "model.add(Bidirectional(LSTM(16))) # dropout=0.2, recurrent_dropout=0.2\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(x_val, y_val))\n",
        "score, acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_19 (Bidirectio (None, 32)                14976     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,015,009\n",
            "Trainable params: 1,015,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.5689 - accuracy: 0.6964 - val_loss: 0.4867 - val_accuracy: 0.7644\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.4412 - accuracy: 0.7943 - val_loss: 0.4050 - val_accuracy: 0.8112\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.4041 - accuracy: 0.8141 - val_loss: 0.3886 - val_accuracy: 0.8222\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3761 - accuracy: 0.8281 - val_loss: 0.3772 - val_accuracy: 0.8258\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3539 - accuracy: 0.8420 - val_loss: 0.3653 - val_accuracy: 0.8308\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3361 - accuracy: 0.8504 - val_loss: 0.3625 - val_accuracy: 0.8326\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3239 - accuracy: 0.8547 - val_loss: 0.3516 - val_accuracy: 0.8390\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.3098 - accuracy: 0.8657 - val_loss: 0.3470 - val_accuracy: 0.8392\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2958 - accuracy: 0.8712 - val_loss: 0.3685 - val_accuracy: 0.8350\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 7s 11ms/step - loss: 0.2831 - accuracy: 0.8773 - val_loss: 0.3467 - val_accuracy: 0.8438\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.3442 - accuracy: 0.8490\n",
            "Test accuracy: 0.8490399718284607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M7MEHJSO22X",
        "colab_type": "text"
      },
      "source": [
        "# **Part 4: New NN architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50EL3cv5PFyH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNZLyrdiPF4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "86ac9fd0-dd74-4d45-957f-6f11547fb61a"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(dictionary_length, embedding_dim, input_length=input_length))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences=True))) # dropout=0.2, recurrent_dropout=0.2\n",
        "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(x_val, y_val))\n",
        "score, acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_20 (Bidirectio (None, 100, 64)           34048     \n",
            "_________________________________________________________________\n",
            "bidirectional_21 (Bidirectio (None, 100, 64)           24832     \n",
            "_________________________________________________________________\n",
            "bidirectional_22 (Bidirectio (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,096,193\n",
            "Trainable params: 1,096,193\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 19s 30ms/step - loss: 0.5607 - accuracy: 0.7060 - val_loss: 0.4779 - val_accuracy: 0.7506\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.4290 - accuracy: 0.8002 - val_loss: 0.4393 - val_accuracy: 0.7866\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3808 - accuracy: 0.8261 - val_loss: 0.3892 - val_accuracy: 0.8224\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3450 - accuracy: 0.8483 - val_loss: 0.3645 - val_accuracy: 0.8354\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3193 - accuracy: 0.8583 - val_loss: 0.3884 - val_accuracy: 0.8200\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3001 - accuracy: 0.8698 - val_loss: 0.3517 - val_accuracy: 0.8432\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 18s 28ms/step - loss: 0.2764 - accuracy: 0.8809 - val_loss: 0.3580 - val_accuracy: 0.8458\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.2521 - accuracy: 0.8916 - val_loss: 0.3554 - val_accuracy: 0.8500\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.2321 - accuracy: 0.9016 - val_loss: 0.4230 - val_accuracy: 0.8310\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.2073 - accuracy: 0.9181 - val_loss: 0.4219 - val_accuracy: 0.8354\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.4177 - accuracy: 0.8357\n",
            "Test accuracy: 0.8356800079345703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMVJI_wiS3XE",
        "colab_type": "text"
      },
      "source": [
        "# **Part 5: DeepMoji**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB7OXRl-TBx5",
        "colab_type": "text"
      },
      "source": [
        "???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nZ_Q4TEWwtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch==1.0.1 -f https://download.pytorch.org/whl/cpu/stable \n",
        "!git clone https://github.com/huggingface/torchMoji\n",
        "import os\n",
        "os.chdir('torchMoji')\n",
        "!pip3 install -e ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOoLJzFRa_gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 scripts/download_weights.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yifU9mcKS3uN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Trains the DeepMoji architecture on the IMDB sentiment classification task.\n",
        "   This is a simple example of using the architecture without the pretrained model.\n",
        "   The architecture is designed for transfer learning - it should normally\n",
        "   be used with the pretrained model for optimal performance.\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "import example_helper\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.datasets import imdb\n",
        "from deepmoji.model_def import deepmoji_architecture\n",
        "\n",
        "# Seed for reproducibility\n",
        "np.random.seed(1337)\n",
        "\n",
        "nb_tokens = 20000\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=nb_tokens)\n",
        "print(len(X_train), 'train sequences')\n",
        "print(len(X_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = deepmoji_architecture(nb_classes=2, nb_tokens=nb_tokens, maxlen=maxlen)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=15,\n",
        "          validation_data=(X_test, y_test))\n",
        "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
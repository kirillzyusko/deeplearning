{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "lab.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "WZfImSn8R0d0",
        "colab_type": "text"
      },
      "source": [
        "<b>Google Colab</b> <a href=\"https://colab.research.google.com/github/kirillzyusko/deeplearning/blob/master/7/lab.ipynb\">link</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml1ygy5_uBJS",
        "colab_type": "text"
      },
      "source": [
        "Authorize google + kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKErrc71uBQb",
        "colab_type": "code",
        "outputId": "11131988-c0ce-4b74-e783-a5672c5cd140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "190Qk0KuuUSa",
        "colab_type": "text"
      },
      "source": [
        "Be sure, that we authorized and have an access to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYY0Nxi_uUZO",
        "colab_type": "code",
        "outputId": "d000253d-d604-41c7-d0c5-f456f51406c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls /content/.kaggle/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;32mkaggle.json\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g0sOvfQO1nc",
        "colab_type": "text"
      },
      "source": [
        "# **Part 1: Download dataset, extract**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-pGLa_TPFH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "72cb4174-6794-4210-b700-bbb201096bf1"
      },
      "source": [
        "!wget \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-03 16:58:45--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  35.0MB/s    in 2.3s    \n",
            "\n",
            "2020-04-03 16:58:47 (35.0 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTyAcu9T83fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!tar xvzf aclImdb_v1.tar.gz -C data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmj29ma282Wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "imdb_dir = 'data/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XCyuGwoO9fs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "612c0119-f9ef-41e6-a617-d97a426c1743"
      },
      "source": [
        "labels.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2iv169Z_GfL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9b0bfd27-ee01-42da-f8ee-121a28fa9698"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # We will cut reviews after 20 words\n",
        "training_samples = 12500  # We will be training on 25000 samples\n",
        "validation_samples = 12500  # We will be validating on 25000 samples\n",
        "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Split the data into a training set and a validation set\n",
        "# But first, shuffle the data, since we started from data\n",
        "# where sample are ordered (all negative first, then all positive).\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 88582 unique tokens.\n",
            "Shape of data tensor: (25000, 100)\n",
            "Shape of label tensor: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i75QPEuGiay",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "3da5df25-06ca-4924-c23d-82381b2a3413"
      },
      "source": [
        "x_train[1]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1329,    2,  122,   46, 3921,    2,    7,    7, 1334, 1080,    9,\n",
              "          7,    7, 6400,  149, 1533,  165,   37,    9,   13,  811,   20,\n",
              "        371,    9,   13,    7,    7,  811,   20, 4455,  341, 5811,    1,\n",
              "       1053,   66,    7,    7, 1853,    4, 1519,   52, 1520, 1853,    2,\n",
              "       7312,    7,    7,  162, 1776,  292,    8,   46,  136,   18,  666,\n",
              "         22,  188,  839,    7,    7,    1,  153,   22,  188,  388,   48,\n",
              "        503,  396,    2,   22,  460,    7,    7,   51,    1,  372,  211,\n",
              "         24,   36,   24,  643,    7,    7,  137,    2,  849,   11,   17,\n",
              "       1424,  105,   37,   11,   33,   23,    3, 6911,    2,    7,    7,\n",
              "          3], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv9aqcMPOzjy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "55cb67ba-5277-4dac-e2c2-1f45b290b387"
      },
      "source": [
        "x_val[0]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  20,  285,   12, 5445,    7,    7,    1,  236,   31, 2272,  248,\n",
              "       2074,    6,   28,    4,    1,  830,  865,  113, 2628,  204,  123,\n",
              "        107,  368, 1998, 1338,    7,    7,   58,  314, 2022, 3309,   50,\n",
              "         71,   46, 7630,    5,    3,   50, 1133,   19,    1, 8116, 4026,\n",
              "          1,    8,   12,  254, 2235,    1,  110,    4,    3,  144,  411,\n",
              "       2962,    1,    2, 6502,    4,   32,  573, 4353, 1742,    7,    7,\n",
              "          2,    8,  396,   35,    9,   16, 1019, 8294,   60,  775,   43,\n",
              "          4,    1,  311,   12, 3382,   69,  325,   14,    1, 4790,   36,\n",
              "       5676,    5, 5676,   10, 1291,  841, 6111,  200,   27,   15,   58,\n",
              "       1352], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Xdl2_LPDxg",
        "colab_type": "text"
      },
      "source": [
        "Download dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cca54DmnPD5S",
        "colab_type": "code",
        "outputId": "765b54b0-e9b1-4c33-9a10-973ab7a12de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews -p /content/kaggle/imdb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content/kaggle/imdb\n",
            " 70% 18.0M/25.7M [00:00<00:00, 28.6MB/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 40.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2hNyXbPvSQa",
        "colab_type": "text"
      },
      "source": [
        "Extract .zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99z2-nHFvT5B",
        "colab_type": "code",
        "outputId": "334afa09-3b25-451a-f938-979a4ea9781d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip kaggle/imdb/imdb-dataset-of-50k-movie-reviews.zip -d data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  kaggle/imdb/imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: data/IMDB Dataset.csv   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3LwHVDEwcLD",
        "colab_type": "text"
      },
      "source": [
        "Files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeETnYp5wby8",
        "colab_type": "code",
        "outputId": "0ce66b82-b5ba-44b3-94c6-171745360d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%ls data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'IMDB Dataset.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8UdGt-Pw8Mf",
        "colab_type": "text"
      },
      "source": [
        "Read data using pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "489LXKpvw8S0",
        "colab_type": "code",
        "outputId": "4c5dff5c-5b93-4e67-c7fd-47806d132ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "imdb_ds = pd.read_csv('data/IMDB Dataset.csv')\n",
        "print(imdb_ds.shape)\n",
        "\n",
        "imdb_ds.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4v4Dy4O10gK",
        "colab_type": "code",
        "outputId": "db739eef-0f02-4961-be23-ddea07755cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/cdipaolo/sentiment/master/datasets/imdb.vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-18 13:31:22--  https://raw.githubusercontent.com/cdipaolo/sentiment/master/datasets/imdb.vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 845980 (826K) [text/plain]\n",
            "Saving to: ‘imdb.vocab’\n",
            "\n",
            "\rimdb.vocab            0%[                    ]       0  --.-KB/s               \rimdb.vocab          100%[===================>] 826.15K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-03-18 13:31:22 (8.77 MB/s) - ‘imdb.vocab’ saved [845980/845980]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsaj3wZ32ACN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"imdb.vocab\", \"r\") as file:\n",
        "  data = file.read()\n",
        "  print(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTBvgFLfuJGd",
        "colab_type": "text"
      },
      "source": [
        "........................................."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuhyIe3TuJSq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "072cc4c1-fc8e-46f5-e40c-3c0b16356d6e"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQD5NYoruxfC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0fAPtGfvapV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "17e3a607-ff9b-4744-e644-ce4cc3e7959b"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB_F6vYSH9_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bf5ed6bb-d506-46a7-ddd4-628dbaa7ace2"
      },
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6etXKKLfO2h5",
        "colab_type": "text"
      },
      "source": [
        "# **Part 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtE3byXjz7lR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr5D5RWWuxRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f79a2ad6-4b50-49d2-f1b0-960066d8b45b"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(VOCAB_SIZE, 8, input_length=maxlen))\n",
        "model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=1)\n",
        "score, acc = model.evaluate(x_val, y_val)\n",
        "\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 66s 168ms/step - loss: 0.6933 - accuracy: 0.5006\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 63s 160ms/step - loss: 0.6826 - accuracy: 0.5716\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.5788 - accuracy: 0.6989\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 62s 157ms/step - loss: 0.4378 - accuracy: 0.8014\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.3267 - accuracy: 0.8586\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 65s 166ms/step - loss: 0.2516 - accuracy: 0.8964\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 62s 160ms/step - loss: 0.1943 - accuracy: 0.9229\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 62s 158ms/step - loss: 0.1577 - accuracy: 0.9402\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 0.1294 - accuracy: 0.9503\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 63s 160ms/step - loss: 0.1114 - accuracy: 0.9570\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.8279 - accuracy: 0.5061\n",
            "Test accuracy: 0.5060799717903137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIrvEkMaO2uP",
        "colab_type": "text"
      },
      "source": [
        "# **Part 3: Using GloVe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlaKV6VOPFCI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MpP3hLeAiT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a013cf91-5727-49da-e219-459d99058ec0"
      },
      "source": [
        "!wget \"http://nlp.stanford.edu/data/glove.6B.zip\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-03 17:06:42--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-03 17:06:42--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-03 17:06:42--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.90MB/s    in 6m 30s  \n",
            "\n",
            "2020-04-03 17:13:13 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gADgatQ0A0sv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "156973f8-ffcd-4cb0-ab53-5cdd17f2cf25"
      },
      "source": [
        "!mkdir glove\n",
        "!unzip glove.6B.zip -d glove"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove/glove.6B.50d.txt  \n",
            "  inflating: glove/glove.6B.100d.txt  \n",
            "  inflating: glove/glove.6B.200d.txt  \n",
            "  inflating: glove/glove.6B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtWg-lOaCHPH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebbf0c27-bb38-4d8d-c672-930ec0199b73"
      },
      "source": [
        "glove_dir = 'glove'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YheEVG1CCPIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXW2oZM9CjIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "84c2a1a5-05a8-405b-ace6-748b3d0f2775"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(VOCAB_SIZE, embedding_dim, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=1)\n",
        "score, acc = model.evaluate(x_val, y_val)\n",
        "\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 20, 100)           1000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_9 (Bidirection (None, 64)                34048     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 1,034,113\n",
            "Trainable params: 1,034,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 20) for input Tensor(\"embedding_9_input:0\", shape=(None, 20), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 20) for input Tensor(\"embedding_9_input:0\", shape=(None, 20), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
            "391/391 [==============================] - 73s 187ms/step - loss: 0.6969 - accuracy: 0.5062\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 73s 186ms/step - loss: 0.6918 - accuracy: 0.5226\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 72s 185ms/step - loss: 0.6901 - accuracy: 0.5279\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 72s 185ms/step - loss: 0.6876 - accuracy: 0.5387\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 72s 185ms/step - loss: 0.6838 - accuracy: 0.5520\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 74s 190ms/step - loss: 0.6820 - accuracy: 0.5549\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 75s 193ms/step - loss: 0.6772 - accuracy: 0.5667\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 75s 192ms/step - loss: 0.6706 - accuracy: 0.5893\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 75s 191ms/step - loss: 0.6640 - accuracy: 0.5937\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 75s 191ms/step - loss: 0.6542 - accuracy: 0.6105\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 20) for input Tensor(\"embedding_9_input:0\", shape=(None, 20), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 0.7168 - accuracy: 0.4974\n",
            "Test accuracy: 0.49744001030921936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M7MEHJSO22X",
        "colab_type": "text"
      },
      "source": [
        "# **Part 4: New NN architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50EL3cv5PFyH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNZLyrdiPF4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "outputId": "050d5692-7805-43bc-b9dc-91b0d4aafaba"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(VOCAB_SIZE, embedding_dim, input_length=max_len))\n",
        "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=1)\n",
        "score, acc = model.evaluate(x_val, y_val)\n",
        "\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 20, 100)           1000000   \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, 32)                17024     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,018,113\n",
            "Trainable params: 1,018,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 20) for input Tensor(\"embedding_13_input:0\", shape=(None, 20), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 20) for input Tensor(\"embedding_13_input:0\", shape=(None, 20), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
            "237/391 [=================>............] - ETA: 15s - loss: 0.6946 - accuracy: 0.5001"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-23bc43f18d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    784\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMVJI_wiS3XE",
        "colab_type": "text"
      },
      "source": [
        "# **Part 5: DeepMoji**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB7OXRl-TBx5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nZ_Q4TEWwtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install torch==1.0.1 -f https://download.pytorch.org/whl/cpu/stable \n",
        "!git clone https://github.com/huggingface/torchMoji\n",
        "import os\n",
        "os.chdir('torchMoji')\n",
        "!pip3 install -e ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOoLJzFRa_gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 scripts/download_weights.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odxKHw-rbl31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7cdfb67-1933-45d1-b1ae-a574ef6cf00a"
      },
      "source": [
        "%cd ../../../../../"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06aqSJSBck8X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f7ab5f4f-90c6-4fdf-8641-c0e357b243aa"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/      emoji_overview.png  \u001b[01;34mexamples\u001b[0m/  \u001b[01;34mmodel\u001b[0m/     \u001b[01;34mscripts\u001b[0m/  \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdeepmoji\u001b[0m/  emoji_unicode.csv   LICENSE    README.md  setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh_S4entdLPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "bc46936c-f449-42ee-94c5-cf0e3c7f1dbb"
      },
      "source": [
        "!python scripts/download_weights.py"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "About to download the pretrained weights file from https://www.dropbox.com/s/xqarafsl6a8f9ny/deepmoji_weights.hdf5?dl=0#\n",
            "The size of the file is roughly 85MB. Continue? [y/n]\n",
            "Traceback (most recent call last):\n",
            "  File \"scripts/download_weights.py\", line 52, in <module>\n",
            "    if already_exists or prompt():\n",
            "  File \"scripts/download_weights.py\", line 30, in prompt\n",
            "    choice = raw_input().lower()\n",
            "NameError: name 'raw_input' is not defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yifU9mcKS3uN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Trains the DeepMoji architecture on the IMDB sentiment classification task.\n",
        "   This is a simple example of using the architecture without the pretrained model.\n",
        "   The architecture is designed for transfer learning - it should normally\n",
        "   be used with the pretrained model for optimal performance.\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "import example_helper\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.datasets import imdb\n",
        "from deepmoji.model_def import deepmoji_architecture\n",
        "\n",
        "# Seed for reproducibility\n",
        "np.random.seed(1337)\n",
        "\n",
        "nb_tokens = 20000\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=nb_tokens)\n",
        "print(len(X_train), 'train sequences')\n",
        "print(len(X_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = deepmoji_architecture(nb_classes=2, nb_tokens=nb_tokens, maxlen=maxlen)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=15,\n",
        "          validation_data=(X_test, y_test))\n",
        "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}